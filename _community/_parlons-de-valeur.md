---
title: 'Parlons de Valeur'
subtitle: la web performance pour les décideur·euses Web & Marketing
description: >-
    Les meilleures pratiques en matière de performances web sont consensuelles et largement acceptées, mais comment les valoriser d’un point de vue commercial ?


last_date: 2023-05-10
events:
    - name: 'We Love Speed 2023'
      url: https://www.welovespeed.com/2023/
      date: 2023-05-10
last_modified_at: 2025-02-01
---

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-0.png" %}

---

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-1.png" containerAttributes='style="position:sticky;top:0px"' %}

Comme Stéphane vient de le dire, je suis Boris et je travaille en effet chez Contentsquare au sein de la division Customer Success, dans laquelle j'occupe le poste de Solution Expert. Un Solution Expert, chez nous, accompagne les clients dans l'analyse des données remontées par notre <span lang="en">Analytics UX</span>. Depuis leurs montées en compétences : sur la Web Performance (WebPerf), sur la gestion de projet transverse ; et jusqu'à leur autonomie, c'est-à-dire leur capacité à associer une valeur aux optimisations qu'ils recommandent à leurs équipes.

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-2.png" containerAttributes='style="position:sticky;top:0px"' %}

Aujourd'hui, nous allons parler du Lab Testing, de ce que cela apporte mais aussi de ce que cela peut avoir comme faiblesses. Nous allons parler de ROI, des limites du ROI et des biais dans lesquels il ne faut pas tomber. Nous allons parler de comment tout cela peut permettre de soutenir la démonstration de valeur.

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-3.png" containerAttributes='style="position:sticky;top:0px"' %}

Je vais commencer par dire un peu d'où je viens, à savoir le Lab Testing. En fait, je suis un ancien salarié de Dareboost, une compagnie qui a été rachetée par Contentsquare et qui proposait un service de Lab Testing pour surveiller la Web Perf.

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-4.png" containerAttributes='style="position:sticky;top:0px"' %}

Dareboost était plutôt apprécié avec un angle de communication transverse entre les équipes métiers. On faisait beaucoup de rédactionnel pour aider les gens à bien comprendre les problématiques de Web Perf. Nous étions bien adaptés au marché français avec un vrai effort de traduction des interfaces et recommendations qui nous positionnait assez naturellement comme un des rares acteurs poussant un peu le sujet de la Web Perf en France.

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-5.png" containerAttributes='style="position:sticky;top:0px"' %}

C'était un outil qui était très apprécié, mais on avait aussi des retours assez réguliers de clients qui avaient du mal à articuler ce que nous leur recommandions avec la manière d'organiser les projets. Cela donnait des témoignages comme :

> Est-ce qu'améliorer ces indicateurs apporte quelque chose à **mon** organisation ?

> Comment **prioriser** ce sujet par rapport à tous les autres ? Je dois aussi travailler sur la sécurité, j'ai travaillé sur l'accessibilité, je dois travailler sur l'évolution des produits. Comment je peux aller dire qu'il faut faire un petit peu de Web Perf ?

Et puis :

> Combien cela va-t-il nous faire **gagner** ? J'ai besoin de le savoir **avant** d'engager des correctifs.

Ces questions sont très, très légitimes quand on comprend comment fonctionne la pérennisation d'un projet d'optimisation (et ce, quel que soit le sujet : dette technique, sécurité, accessibilité…).

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-6.png" containerAttributes='style="position:sticky;top:0px"' %}

Dans beaucoup de structures, les **problèmes d'utilisabilité** ne sont abordés qu'une fois qu'ils sont **constatés**. Il faut que la frustration, la friction initiale, soit ressentie et exprimée par quelqu'un d'assez important pour initier une démarche corrective.

On projette ensuite une **valorisation** sur les gains d'une potentielle optimisation, c'est-à-dire qu'une équipe est missionnée pour évaluer le **coût de non-qualité**. On espère en général que l'investissement soit inférieur au coût, mais ce n'est pas systématique : l'investissement peut être supérieur parce qu'on cherche à résoudre un problème sur plusieurs années.

Cette valeur permet d'évaluer **l'investissement** à apporter. Choisir comment investir n'est pas une mince affaire, même une fois le problème bien identifié. Plusieurs méthodologies, produits ou services disponibles sur le marché peuvent répondre au besoin, et correspondre ou non au niveau de maturité et de disponibilité des équipes.

Une fois **l'investissement réalisé**, on constate ses effets. C'est la phase d'évaluation du **retour sur investissement**.

Enfin si la démarche est vertueuse, on en assure **le financement**. C'est une étape qu'on oublie souvent, alors qu'elle peut durer plusieurs mois. Les parties-prenantes doivent trouver comment **ventiler les budgets**. Entre les différentes divisions, mais souvent aussi entre **le coût d'acquisition ou de licence** des logiciels et le coût de la main d'œuvre associée à son déploiement, son utilisation, et sa maintenance.

---

Entre chacune de ces étapes, il y a beaucoup de **travail d'expérimentation**, qui fournit les réponses permettant de passer d'une étape à l'autre.

Prenons **le passage de la friction à la projection de valeur**. L'exercice est difficile et si l'organisation n'a pas accès à des données en propre, elle peut essayer de sensibiliser des décideurs avec des **arguments d'autorité** (comme les citations de Google sur le rôle de la web performance dans le classement sur les résultats de recherche) et des cas d'usage issus de clients similaires.

Pour **bascule de la valorisation l'investissement** : des tests en propre seront nécessaires. Cela peut passer par une expérimentation réalisé par une personne de l'organisation dont il faudra assurer la montée en compétence.

---

Si on avait placé sur le diagramme notre solution de Lab Testing Dareboost, elle aurait été en bas, entre la décision d'investir et l'évaluation du retour sur investissement. Une grande partie du travail avait été fait en amont :

- les parties- avaient déjà détecté que la performance web devait être améliorée ;
- des développeurs ou développeuses intéressées par le sujet avaient déjà identifié notre solution et étaient prêt·es à l'utiliser.

Mais ça ne suffisait pas à péreniser l'usage.

{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-7.png" containerAttributes='style="position:sticky;top:0px"' %}

En effet, pour emmener une organisation de l'expérimentation à la pérennisation, il y a trois défis à relever.

D'abord, **le défi opérationnel** : trouver des compétences, des solutions organisationelles et des solutions logicielles. 

audits, de tests budgétaires, des histoires de démonstration de ROI, de qualifications, de ventilation sur les différentes entités, de priorisation du budget opérationnel versus budget d'investissement aux PEPs, et puis des problématiques de gouvernance. Comment, dans mon entreprise, je vais arriver, si je veux que tout ça soit pérennisé dans le temps et dure plusieurs années, à créer une culture, des procédures, des fréquences d'usage, des rapports ? À la fin de la conférence, je pointe aussi d'autres conférences qui ont eu lieu par le passé. On a plein de gens qui sont venus témoigner à Will of Speed de la manière dont ils organisent leur sujet de performance, et c'est probablement en réalité le sujet le plus complexe à traiter.

Une fois qu'on a eu tout ça, ben nous, on s'est dit : "On peut peut-être faire quelque chose." Mais pour faire quelque chose, il faut qu'on arrive à mettre de la valeur sur la Web Perf. Et pour mettre de la valeur sur la Web Perf, du coup, il fallait qu'on ajoute des briques à notre solution.

Dareboost a été racheté par Contentsquare, et quand on a été racheté par Contentsquare, avec moi, j'ai rejoint les équipes Customer Success de Contentsquare. Je me suis rendu compte qu'en fait, ce sujet était très, très bien traité là-bas, très bien traité notamment parce que c'était pas du tout les mêmes types de clients. On parlait beaucoup plus aux divisions digitales marketing, sur lesquelles le focus valeur était beaucoup plus important que chez les développeurs, sur lesquels on a une culture de la bonne pratique qui est extrêmement pertinente, mais qui du coup ne nous permettait pas d'avancer sur ces sujets-là.

Donc, du coup, je me suis beaucoup inspiré de ce que faisait Contentsquare dans ces sujets-là. Et pour vous en parler, je suis obligé de dire un tout petit peu ce que fait Contentsquare. Contentsquare fait quoi ? Contentsquare fait partie du principe que hors ligne, c'est assez facile de discuter avec quelqu'un et de comprendre ce que cette personne ressent, parce que quand vous voyez quelqu'un hors ligne, bah, vous interprétez un certain nombre de signaux en fait que cette personne aime. Vous savez si elle est frustrée, si elle est joyeuse. Quand vous analysez vos Analytics, en fait, vous ne savez pas du tout ce que les gens ont essayé de faire. Vous avez très peu d'informations sur les raisons qui les ont poussés à être ici. Je donne un exemple un petit peu plus tard concret, et donc du coup, c'est un peu difficile. Donc, dans Contentsquare, ce qu'on fait, c'est essayer de capter un petit peu ce que les personnes viennent faire sur les sites et de donner des outils d'analyse ou d'analystes pour permettre de traiter et d'analyser en fait cette collecte de données qu'on réalise, qui du coup n'a pas besoin d'être personnel. Le but, c'est de savoir qui sont les gens, le but, c'est de savoir pourquoi ils sont là.

Concrètement, ça se fait avec un tag JS posé sur un site qui va aller collecter un certain nombre d'indicateurs de performance. Donc, là, c'est performance, pandémie, Web Performance, performance globale. Est-ce que les gens sont frustrés ? Quel est leur contexte ? Quelles sont les anomalies d'utilisabilité sur ce que font les gens avec leur curseur de souris, sur ce qu'ils font à l'intérieur des formulaires, de produits, de contenu pour voir comment les gens ont cliqué, quels sont les revenus qui ont été générés par zone ? Mais ce qui m'intéresse particulièrement dans cette présentation, par rapport à ce sujet qui est celui de la Web Perf, c'est ce que j'ai mis en gras : arriver à détecter la réactivité des pages, arriver à détecter les hésitations des gens, et arriver à détecter l'attractivité des contenus. Trois éléments qui sont très corrélés avec ce qu'on connaît dans le domaine de la Web Perf, c'est-à-dire les Core Web Vitals, qui sont aussi des indicateurs qui permettent d'aller collecter l'expérience des gens, en tout cas de la projeter de manière à savoir s'ils ont eu une bonne expérience d'utilisation.

Comment on fait ça avec les Core Web Vitals ? C'est la partie où je fais la petite intro pour tous les gens qui connaissent pas encore très bien le sujet de la Web Perf, et qui du coup vont avoir les informations pour le reste de la journée. Basiquement, on cherche à analyser trois ressentis bien précis lors du chargement et de l'interaction avec une page. D'abord, on va se demander si le retour visuel des utilisateurs, utilisatrices, est stable. Pourquoi on fait ça ? Parce que quand vous essayez d'interagir avec quelque chose, que ce soit un site web ou n'importe quoi dans votre vie, vous allez essayer d'interagir avec quelque chose qui bouge pas. Si je vous demande d'aller griller une tartine de pain avec le grille-pain, il saute sur place, vous allez pas griller la tartine de pain, vous allez attendre qu'il arrête de sauter, grille, et c'est la même chose en fait sur un site web. Si je vous demande d'aller interagir avec un contenu, vous allez attendre que ce contenu soit visible et qu'il soit un élément où il est prévisible que vous puissiez interagir avec, sans que vous cliquiez au mauvais endroit. Donc, ça, c'est le premier élément qu'on va chercher à mesurer.

Le deuxième élément qu'on cherche à mesurer, ce sont les marqueurs de confiance. Est-ce que les utilisateurs, utilisatrices, voient quelque chose auquel ils peuvent avoir confiance et qui leur permet d'aller interagir avec le site ? Parce que si je commence à vous afficher des contenus, mais que ça vous intéresse pas du tout, et ben, vous allez pas interagir, parce que vous allez attendre le contenu pour lequel vous êtes là. Une fois que les utilisateurs, utilisatrices, voient ces marqueurs de confiance, alors ils vont commencer à interagir avec la page. Et le troisième élément de l'UX qu'on essaie de capter en fait avec les Core Web Vitals, c'est les interactions sans couture. Une fois que les gens ont interagi en fait avec la page, est-ce que concrètement cette interaction était qualitative ?

Ces trois moments qu'on essaie de capter, Google a fait une proposition d'indicateurs pour les récupérer, et on a trois indicateurs qui correspondent à chacun de ces éléments-là. Pour la stabilité, c'est le Cumulative Layout Shift (CLS). Je vais pas faire toutes les définitions. Pour les marqueurs de confiance, c'est le Largest Contentful Paint (LCP), et pour l'interactivité, c'est le First Input Delay (FID). Ces indicateurs sont par définition imparfaits. Ce sont des approximations de ce qu'on essaie de mesurer. Si je prends par exemple les marqueurs de confiance, concrètement, le LCP, c'est le moment où est apparu le moment, l'élément le plus important visuellement sur la page. Est-ce que c'était pour autant l'élément pour lequel moi je suis venu ? Peut-être pas, peut-être ça n'a rien à voir, mais c'est ce qu'on a de plus proche, et c'est ce qui nous permet d'essayer de projeter des choses, mais ça reste un indicateur imparfait. Le FID, c'est pareil. On mesure le temps nécessaire au navigateur entre le moment où l'utilisateur ou l'utilisatrice a réalisé une interaction active et le moment où le navigateur était capable de la traiter. Concrètement, ça ne veut pas dire que le navigateur a fait quoi que ce soit en termes de retour visuel à l'utilisateur pour lui dire que c'est vrai. Il y a d'autres indicateurs dans les tiroirs. Ce soir, c'est Google qui sait. Ça se trouve, ce soir, cette slide sera complètement à dégommer parce que les Core Web Vitals ne seront plus les mêmes, mais la slide d'avant sur ce qu'on essaie de mesurer, elle n'aura pas changé. Concrètement, on essaie de mesurer la même chose, c'est juste comment on le mesure qui va évoluer.

Donc, nous, on collecte tout ça aussi avec Contentsquare, et donc ça nous permet de faire des analyses où on va aligner à la fois les Core Web Vitals, mais aussi les données des UX Analytics qu'on avait avant et qui nous permettaient d'analyser tous les éléments dont j'ai parlé sur les souhaits et les envies des gens sur les sites, les raisons pour lesquelles ils sont là. Et ça nous a permis assez instinctivement d'aller commencer à faire des choses intéressantes avec ces indicateurs, et c'est ça que je voulais partager avec vous, non pas pour faire la publicité de Contentsquare, mais parce que je pense qu'il y a beaucoup de choses qui sont refaisables sur quasiment n'importe quelle structure, dépendamment de évidemment l'investissement que vous allez y mettre. Moi, par exemple, j'ai réalisé un certain nombre de choses qu'on voit ici dans Contentsquare. Je les ai réalisées avec Matomo sur mon propre site en faisant des captures en fait d'événements de certaines choses et en faisant des corrélations. Ce qui m'a juste intéressé quand j'ai découvert ça, c'est que j'ai découvert genre littéralement découvert que je pouvais faire ça, et j'avais pas pensé, et c'est ça que je trouve intéressant à partager avec vous aujourd'hui.

Un exemple, par exemple, et c'est le premier que nous, utilisateurs, nous ont dit : "Bon, moi, j'aimerais savoir ce qui se passe pour les gens qui ont pas une bonne Web Perf. Concrètement, si j'essaie de comparer, par exemple, d'un côté les gens qui ont une bonne Web Perf, puis d'un côté les gens qui ont pas une perf, concrètement, ça change quoi en fait pour eux ?" Et donc, pour ça, dans les outils de Contentsquare, on a des outils qui permettent de séparer les sessions en fonction de la performance web des gens. Donc, on va être capable de segmenter. Donc, là, c'est un exemple de segmentation sur le LCP. Je prends les utilisateurs qui ont connu un LCP de plus de deux secondes et demie sur n'importe quelle page, et je me fabrique un segment de trafic où là, dans cet exemple, je suis à 24,6 % du trafic analysé. C'est assez peu, mais faut se rappeler que les indicateurs qu'on regarde ne sont collectés qu'auprès des navigateurs Chrome, Chromium. Donc, on est sur du Chrome, du Chromium, du Samsung Browser. On n'est pas non plus sur l'ensemble du marché, et puis suivant le site pour lequel vous travaillez, vous avez des spécificités. Il y a des sites, par exemple, notamment dans le domaine du luxe, où on sait qu'il y a un très grand nombre de trafic qui vient d'iOS, et sur iOS, il y a pas de Chrome. Donc, on n'a pas ces infos-là.

Ce premier outil, il est intuitif. Il permet d'assez rapidement construire une vision encore de l'expérience, je vois des gens qui prennent les photos. Attendez une série d'après où j'explique que c'est pas comme ça qu'on fait. Il permettrait très rapidement de se construire une vision en cours de l'expérience et puis de se faire un premier exemple de ressenti. Un autre truc aussi que j'ai beaucoup utilisé, c'est pour le Lazy Loading. Le Lazy Loading, donc pour celles et ceux qui seraient pas, c'est cette technique qui consiste à donner les informations à l'intérieur du cœur de la page pour permettre au navigateur de savoir s'il peut décharger ou plutôt retarder le chargement de certains éléments, principalement des images ou des iframes. En gros, vous avez une page avec beaucoup d'images, et puis vous lui dites en fait : "Toutes ces images-là, tu les charges en vrai quand tu arrives sur la page, mais toutes celles qui sont par exemple en dessous d'un certain seuil ou cachées derrière une interaction, tu les chargeras soit lors d'une interaction ou d'un scroll, soit un petit peu plus tard dans le chargement de la page." Ce qui fait que, au moins, la partie visible sera visible plus rapidement, et c'est une super fonctionnalité qui est même gérée nativement par les navigateurs. Le problème, c'est que ben, en fait, on la met en place, puis on sait pas ce que ça fait concrètement. Il y a assez peu de gens en fait qui se posent la question : "Une fois que j'ai mis en place le Lazy Loading, ok, j'ai amélioré ma Web Perf, mais est-ce que les gens voient quand même les images ? Est-ce qu'ils voient quand même les iframes ? Est-ce que c'est toujours aussi attractif ?" Et avec des outils de Heatmap, donc là, je montre Contentsquare, mais vous pouvez imaginer utiliser Hotjar ou n'importe quelle autre solution de Heatmap que vous avez à votre disposition, vous pouvez réaliser ce genre de choses avec des segmentations. Donc, là, j'ai pas segmenté par Web Perf, mais j'ai segmenté par fonctionnalités. Donc, à gauche, j'ai les gens qui n'ont pas le Lazy Loading, et à droite, les gens qui ont le Lazy Loading. Sur les lignes 2 et 3, vous pouvez évaluer l'impact dans les îlots. Là, par exemple, moi, quand je regarde les deux carrés qui sont en dessous, ce que j'arrive à déduire, c'est que, en ligne 2, concrètement, avec le Lazy Loading, les blocs sont moins attractifs, parce que je regarde, c'est le taux d'attractivité, c'est-à-dire le nombre de clics pour les gens qui ont vu les éléments. Donc, la ligne 2 est moins attractive. Ça, ça peut me donner une indication, par exemple : "Peut-être mon Lazy Loading est mal géré. Peut-être qu'en fait, j'ai isolé cette ligne 2, alors qu'en fait, j'aurais pas dû. J'aurais dû charger ces éléments au plus vite de manière à moins retarder leur affichage et que les gens puissent interagir plus rapidement." Par contre, ce que je vois, c'est que la ligne 3, elle, elle est beaucoup plus attractive qu'avant. La ligne 3, donc celle qui est en violet foncé là, elle est à 0,28 % de taux d'attractivité contre 0,08 %. C'est presque un facteur 3. Donc, là, ça m'indique que même si la ligne 2 doit être peut-être touchée un petit peu, concrètement, la ligne 3 bénéficie de mon Lazy Loading, puisque il y a de plus en plus de gens qui vont la voir et interagir avec.

C'est avec ce genre d'outils, vous allez pouvoir choisir, par exemple, si vous voulez rester sur le Lazy Loading natif, qui va charger un certain nombre de hauteur d'image, ou si vous voulez partir sur un Lazy Loading custom. Vous allez pouvoir choisir, vous mettre votre seuil, et éventuellement d'autres facteurs pour rendre l'expérience un peu plus fluide. Ça, c'est un autre exemple de corrélation entre les Analytics UX et la Web Performance.

Un autre exemple : est-ce qu'il y a des gens parmi vous qui ont déjà établi des recommandations sur le nombre d'éléments à l'intérieur d'une liste ? Vous savez, des fois, vous avez des listes paginées, et puis on vous dit : "Bah non, concrètement, 70 produits sur la page, c'est beaucoup. Quand même, donc peut-être on va en mettre que 12." Concrètement, comment vous faites pour évaluer si ce que vous avez recommandé, et une fois que cette recommandation est en place, si elle est positive ? Qu'est-ce qui s'est passé ? Est-ce que les gens voient toujours autant de produits ? Est-ce qu'ils en voient moins ? Est-ce que ceux qui voient, en fait, ils continuent à interagir avec ? Ben, c'est là aussi où on peut mettre une valeur sur ce qu'on a fait en termes de Web Perf. On peut aller regarder. Ici, c'est la façon dont on fait donc dans Contentsquare, c'est ce qu'on appelle le Journey Analysis ou le Sunburst. C'est le parcours des gens concentré. Que le rond vert du milieu étant la liste produit, et les ronds concentriques successifs étant les autres pages qui ont été visitées. En soit, cette représentation m'intéresse pas pour ce que je voulais montrer. J'ai mis des carrés spécifiques pour dire ce que je voulais montrer, à savoir que, une fois qu'on a optimisé la liste produit, concrètement, on a un taux de rebond qui est largement réduit. Le taux de rebond sur ces visites produits est passé de 23,8 % à 23 %, et en revanche, on a un nombre de gens, en fait, qui itèrent sur les listes produits qui est beaucoup plus important. On avait 37,8 % des gens qui allaient voir une nouvelle liste produit à partir de celle où ils étaient. Maintenant, on passe à 40,7 %. Ce qui est assez logique, puisque, puisqu'on aurait réduit le nombre de produits sur la liste, littéralement, les gens qui veulent voir les produits suivants sont obligés de paginer. Avec ce type de visualisation, on est capable de prouver qu'en effet, ils ont pas gêné, et si on voulait aller plus loin, on pourrait regarder à la page 2 combien ils ont vu de produits, ce qu'ils étaient aussi attractifs pour essayer de confirmer si oui ou non, concrètement, ce qu'on a fait en termes de Web Perf bénéficie en fait au site web, ou si en fait, ça bénéficie uniquement autant de chargement de cette liste produit, mais sans forcément de bénéfice pour les utilisateurs et pour les gens qui vous payent pour optimiser les sites.

Est-ce que pour autant utiliser du RUM et des UX, c'est une solution magique ? Ben non. Moi, j'étais hyper ravi, évidemment. J'ai fait plein de trucs, et tout. J'ai préparé mes slides. J'ai commencé à présenter des trucs à des clients, et puis très rapidement, on est tombé sur des trucs qui nous semblaient pas du tout cohérents, mais pas du tout. C'était vraiment extrêmement difficile, et pendant des mois, en fait, on a trouvé des corrélations qui n'avaient aucun sens. Et c'était une des raisons pour lesquelles je voulais partager aussi ça avec vous aujourd'hui, et vous donner en fait quelques pièges dans lesquels ne pas tomber si vous décidez un jour de faire ce que je viens de montrer.

Premier piège : attention aux échelles de temps. Un site web, ça vit, et ça vit beaucoup. Et quand vous livrez en fait des fonctionnalités de collecte de l'expérience des vrais utilisateurs, vous dépendez des vrais utilisateurs. Si on prend une semaine un peu chaotique où vous partez d'un week-end où il y a assez peu de sessions, puis le lundi, vous faites une campagne sur des gens qui sont plus CSP+ avec du bon matériel, avec des bonnes connexions, et puis le mardi, vous mettez le site en maintenance, et vous croyez pas qu'un site en maintenance, ça convertit pas. Par contre, c'est très rapide, la page de maintenance. C'est vraiment super hyper optimisé. Donc, là, vous avez des supers indicateurs, par contre, évidemment, vous avez des valeurs derrière qui sont pas bonnes du tout. Et puis, vous faites une pub à la télé, ou votre organisation fait une pub à la télé. Donc, là, d'un seul coup, il y a plein de gens qui arrivent. Ça ralentit le serveur, mais les gens aussi arrivent avec un contexte de navigation qui est très différent, parce que du coup, avec cette pub, vous allez toucher des gens qui ont des contextes de navigation qui sont très, très variés, beaucoup plus que dans votre campagne CSP+ de jour avant. Et puis, un jour, vous avez un problème de Content Management Platform, et puis vous chargez tous les JavaScript par erreur, parce que évidemment, d'abord, un problème légal RGPD, mais ce qui est aussi un problème de Web Perf. Et du coup, vos données sont extrêmement biaisées. Là, évidemment, c'est une semaine un peu chaotique, mais même en vrai, si votre site, il vit pas beaucoup autant que ça, concrètement, une semaine, ça reste une semaine. Et les gens, quand ils sont au travail, ils ont pas les mêmes connexions que chez eux. Quand ils sont dans le bus, ils ont pas la même connexion que la table du petit déj. Et donc, si vous prenez des périodes de temps qui sont trop courtes, notamment de l'ordre inférieur à une semaine, voire deux, concrètement, vous allez vous retrouver avec des jeux de données qui ne sont absolument pas comparables, et vous allez essayer de tirer des conclusions de choses qui ne sont absolument pas comparables. Donc, attention à ça. Privilégiez des plages de temps qui sont assez importantes, et si vous travaillez comme nous, on le fait avec des objectifs utilisateurs en segmentant avec des objectifs. Essayez de trouver des objectifs qui sont proches de là où vous êtes en train d'analyser. Typiquement, si je suis sur une liste produits, mon objectif, c'est pas la conversion panier. Mon objectif, c'est : est-ce que l'utilisateur, il a vu un produit, parce que c'est un objectif court qui me permet d'analyser vraiment la pertinence de la liste produit. Si je suis sur un produit, je peux avoir plusieurs objectifs qui peuvent être la mise au panier, qui peuvent être la consultation d'un produit associé, qui peuvent être évidemment les indicateurs de perf, mais a priori, si vous êtes là, c'est que vous êtes déjà un peu convaincus. Mais essayez de trouver des indicateurs qui sont vraiment reliés à ce que vous êtes en train de regarder.

Un autre biais auquel il faut faire attention, c'est que on travaille avec des indicateurs qui changent au fil d'une session. Si je prends trois utilisateurs ici : Alice, les Bakens, et Chabir. Concrètement, ces trois utilisateurs sont allés sur mon site avec des contextes de navigation très différents. Alice, elle a un vieil appareil qui a une dizaine d'années. Elle capte le Wi-Fi de la bibliothèque municipale. C'est pas mal, mais son ordinateur est en collant. Les Bakens, c'est un couple qui est en train de chercher un produit très spécifique sur Google, et ils vont aller ouvrir directement en fait des pages qui correspondent aux produits qu'ils cherchent. Eux, ils ont une très, très bonne connexion avec du très bon matériel, et quand ils ouvrent les pages, tout de suite, une très bonne expérience. Et Chabir, mon troisième exemple, lui, il est dans un bus. Il est en vacances. Il a acheté en fait des cadeaux qu'il veut offrir à ses proches, et il veut retourner sur le site pour voir concrètement s'il s'est pas trompé dans son achat, parce qu'il vient d'apprendre qu'il fallait acheter le bidule bleu et pas très sûr s'il a bien pris le bleu ou plutôt le rouge.

Ces trois utilisateurs vont avoir des expériences sur mon site qui vont être très différentes en termes de, ici, j'ai pris le LCP, en termes de pages sur lesquelles ils sont tombés, et en termes d'expérience sur ces pages. Par exemple, Chabir, ayant une connectivité qui est très variable à l'intérieur de son bus, sur la première page produit qu'il a vue, il a eu un LCP de 10 secondes, mais sur la deuxième, il a eu un LCP de 2 secondes. Son LCP moyen n'est pas si mauvais que ça, en tout cas, il est pas aussi mauvais comparé au LCP de Alice, alors qu'elle n'a vu qu'une seule page. Concrètement, c'est très compliqué de comparer ces trois utilisateurs, parce que leurs objectifs sont pas du tout les mêmes, les modalités qu'ils empruntent pour faire ce qu'ils font ne sont pas du tout les mêmes, et les contextes de navigation de ces trois utilisateurs ne sont pas du tout les mêmes. Donc, si je veux faire une vraie analyse, concrètement, il faut que je sois capable de segmenter pour dire précisément ce que je veux voir. Je veux aller regarder la Web Perf des gens qui sont tombés directement sur ma fiche produit. Je veux aller regarder la Web Perf des gens qui sont tombés directement sur ma page, où je veux aller regarder la Web Perf qui sont tombés sur la fiche produit après avoir visité déjà deux fiches produits. Pourquoi ? Parce que du coup, la Web Perf sera pas du tout la même, et les LCP moyens seront pas du tout les mêmes. Donc, si vous faites ce genre d'analyse, n'hésitez pas à pas du tout tomber dans ce piège, d'autant que, et c'est là où il se passe des choses parfois un peu bizarres. On peut aussi tomber sur des gens qui reviennent voir des produits parce qu'ils les ont déjà achetés, et ils ont une très, très bonne perf, et ils convertiront jamais parce qu'ils ont déjà acheté les produits. Donc, si vous commencez à faire de la segmentation sur : "Est-ce que les gens qui ont la meilleure perf convertissent le mieux ?" Concrètement, la réponse, c'est non. Les gens qui ont la meilleure perf, c'est ceux qui ont déjà converti. Donc, si vous êtes pas capable de segmenter ces gens-là, concrètement, vous allez pas tomber du tout sur ce que vous cherchez à prendre.

Donc, ça veut dire qu'il faut mieux segmenter, et nous, ça nous a amenés à utiliser une segmentation supplémentaire qui consiste à regarder une page spécifique du trafic, notamment ici, dans cet exemple, la page d'entrée. On se dit : "Bah tiens, on va regarder la Web Perf. On va approximer la Web Perf globale à partir de l'expérience initiale des gens quand ils rentrent en fait sur les pages." Je vais pas creuser davantage ce sujet, mais je vous invite à aller voir la conférence de Philippe Tellier, puisque c'est exactement la même conclusion à laquelle ils sont parvenus chez Akamai, et lui, c'est un sujet sur lequel il fait toute la conférence est basée là-dessus. Ce qui est intéressant avec cette démarche, c'est qu'elle est aussi compatible avec les Single Page Applications, puisque si certains d'entre vous ont des applications en Single Page Application, c'est-à-dire un framework JavaScript qui capture les interactions de manière à produire des soft navigations plutôt que des vraies navigations utilisateurs. Concrètement, à date, la collecte des Core Web Vitals ne fonctionne que sur la première page. Ce qui est un vrai piège, puisque c'est pas forcément la première page d'un tunnel. Vous pouvez totalement avoir des utilisateurs, vous pouvez collecter des données à la quatrième page d'un tunnel, même avec les Core Web Vitals. Il explique, la personne est fermée, son ordinateur, puis les roues vertes, ça recharge la page, et vous avez de la donnée enfin.

N'oubliez pas aussi de segmenter ce que vous voulez regarder. Là, par exemple, dans cet exemple, c'est pas bon, puisque je suis en train de regarder le LCP sur l'ensemble des pages de mon site. Ce qui veut dire que je vais aussi analyser l'expérience des gens qui sont rentrés sur mon site par exemple par la page "Mon historique de commandes". Les gens qui ont reçu un email de confirmation compliqué sur le lien, ils arrivent là. Concrètement, c'est pas forcément les gens que je peux regarder si je veux associer une valeur monétaire à ce que je suis en train de faire en termes de Web Perf, parce que ces gens-là, a priori, ne vont pas recommander tout de suite.

Attention aussi au bien de marché. Chaque marché sur lequel vous allez travailler va se comporter différemment. Donc, là, on a, nous, chez Contentsquare, un Digital Experience Benchmark Report où on prend les pétaoctets de données qu'on a pour essayer de produire des visions par marché. Mais à gauche, je vous ai donné quelques exemples de marché. Dans le retail, par exemple, il y a une concurrence assez importante sur la vente de produits avec une très grosse corrélation, en tout cas, nous, ce qu'on a observé entre la performance web et la compétitivité et le gain sur des objectifs comme le taux de rebond, comme la conversion à terme. On a quelque chose finalement qui est assez facile à analyser du point de vue de la Web Performance, d'autant plus facile quand les retailers ont des Marketplaces, c'était parce que du coup, ils ont des quantités de produits qui sont extrêmement importantes. Et quand vous allez optimiser la Web Performance, vous allez permettre au robot d'aller naviguer plus de pages et d'indexer plus de pages. Et donc, vous avez beaucoup travaillé votre longue traîne. Donc, là, c'est un truc qui est assez facile à valoriser auprès des Market et des digitaux, parce qu'ils voient concrètement qu'il y a de plus en plus de visites sur les sites avec des conversions sur les longues traînes qui sont très faibles, mais qui sont existantes. Et le cumul de ces conversions qui n'existait pas avant génère du profit.

Sur le drive, quand vous faites vos courses dans un drive, concrètement, c'est pas du tout la même chose. Moi, quand je choisis pour faire mes courses au drive, je choisis le supermarché qui est le plus proche de chez moi, et celui qui est plus loin, en fait, il est 40 km plus loin. Donc, je vais pas y aller, même si le site, il est pas très rapide. Et donc, si vous avez cherché à faire des corrélations entre la Web Perf et le taux de conversion sur un drive, va pas être du tout la même chanson. Vous avez pas du tout le même type d'utilisateur, et leurs contraintes sont beaucoup plus fortes. Donc, là, vous allez avoir des choses qui sont beaucoup plus difficiles à prouver, et vous avez peut-être intérêt à vous assortir sur des valeurs qui sont liées à des objectifs qui sont pas les objectifs monétaires, mais typiquement, par exemple : est-ce que les gens sont facilement capables d'aller reconsulter le contenu de leur panier ? Est-ce qu'ils sont facilement capables d'aller ajouter une carte de fidélité ? Est-ce qu'ils sont facilement capables ? Sur ces objectifs-là, c'est plus simple, et là, concrètement, c'est des actions que les gens ne peuvent peuvent décider de ne pas faire, par exemple, ajouter la carte de fidélité. Si c'est trop lent et trop pénible, les gens vont partir, et donc là, vous avez un facteur sur lequel vous pouvez valoriser la voiture. Et puis, évidemment, vous allez services exclusifs des objectifs de conversion ou transformation qui sont vraiment liées à des services dans lesquels vous ne pouvez pas vous passer. Donc, là, on est vraiment sur le cas extrême. Concrètement, si le site des impôts est un peu plus lendemain, vous allez quand même déclarer vos impôts.

Une fois qu'on a dit tout ça, l'objectif à terme, c'est d'arriver à quantifier une valeur, c'est-à-dire : j'ai fait des analyses, j'ai fait des cohortes d'utilisateurs, et je voudrais savoir concrètement si les gens qui ont une meilleure Web Perf, ils produisent quelque chose d'intéressant pour mon organisation. C'est évidemment très dangereux comme exercice, puisque le but, c'est d'être data-driven, et vous pouvez totalement arriver à la conclusion que non. Quoi qu'il en soit, ce serait quand même une bonne conclusion, puisque c'est les data qui nous aident à la sortie. Donc, nous, chez Contentsquare, on a un outil dédié en fait de quantification d'impact où on peut mettre en fait des segmentations à droite à gauche, et puis on essaie de comparer des cohortes comparables. Et là, un autre truc, dans un autre piège dans lequel on est bien tombé à pieds joints pendant nos phases de bêta, c'est que bah du coup, les gens disaient : "Génial, les gens qui ont une mauvaise Web Perf, on va augmenter, et puis ils ont une bonne Web Perf, qu'ils ont le même taux de conversion que les autres." Bah non, vous allez pas aller chez eux changer leur connexion, puis vous allez pas changer leur téléphone ou leur ordi. Donc, concrètement, vous allez jamais changer la Web Perf de tous les gens qui ont une mauvaise Web Perf. En revanche, vous pouvez prendre un pourcentage, dire : "Voilà, si je prends 10 % des gens qui ont une mauvaise Web Perf, puisque eux, je peux les amener vers une bonne Web Perf, et si en fait, 10 %, parce que le trafic est déjà très optimisé, le site déjà très rapide. Bah, c'est ambitieux. Vous prenez 1 %, et vous essayez de partir sur des objectifs qui sont plutôt pessimistes. Mieux vaut être pessimiste et mieux réussir que être trop optimiste et de se voir." Un autre facteur à prendre en compte aussi, encore une fois, c'est qu'ici, mais pourcentages de population sont très faibles. Ici, j'ai 24 % qui ont une mauvaise Web Perf, 21 qui ont une bonne. Ils sont où, les autres ? Ils utilisent pas Chrome, mais en même temps, ils sont là quand même. Donc, si j'améliore la Web Perf, est-ce que ça améliorera la Web Perf pour eux aussi ? C'est honnêtement un sujet très complexe sur lequel j'ai pas de réponse. Ça dépend profondément de ce qu'on fait pour améliorer la Web Perf. Si vous optimisez vos images, par exemple, concrètement, ça sera aussi optimisé pour les utilisateurs de Safari. En revanche, si vous allez utiliser des navigateurs de pré-fetch, des instructions de pré-fetch qui sont pas forcément complètement compatibles ou qui font pas exactement la même chose sur d'autres navigateurs, alors du coup, votre optimisation, elle aura pas forcément d'impact sur le trafic là. Donc, c'est aussi un facteur à prendre en compte, et peut-être à rediriger, se dire : "Ok, je vais essayer d'obtenir en fait mon gain potentiel qui est une projection parfaitement parfaite, et j'en suis complètement conscient, mais qui est une projection, et je vais essayer de la diviser par mon trafic de manière à avoir une projection sur le trafic global et me rendre compte concrètement."

Une fois qu'on a toutes ces valeurs-là, ça facilite beaucoup les enjeux. Donc, j'ai parlé précédemment, opérationnellement, ça va nous aider à prioriser les recommandations, parce que une fois que vous êtes capable de créer des segmentations entre les gens qui ont eu un bon LCP, un mauvais LCP, un bon CLS, un mauvais CLS, vous allez être capable déjà de savoir sur quel indicateur vous allez vous mettre en premier. Vous pouvez faire l'analyse typologie de page par typologie de page pour vous rendre compte de la valeur que vous pourriez potentiellement gagner à optimiser la Web Perf sur cette typologie de page. Ce qui vous permet de d'organiser votre backlog et de savoir par quoi commencer. Donc, opérationnellement, ça facilite vraiment l'organisation, et puis ça permet de justifier certains exemples comme des A/B tests en disant : "Voilà, j'ai un truc, je pense que ça a de la valeur. Est-ce qu'on peut financer un A/B test pour le prouver ?" J'ai un exemple juste après la BTS.

Ça facilite évidemment les discussions budgétaires, puisque ça permet de projeter ou de confirmer une fois que la réalisation est faite, l'optimisation est faite, le ROI pour mieux naviguer à l'intérieur des gabarits de financement des différentes solutions. Et puis, enfin, ça facilite la gouvernance, parce que du coup, vous avez un sujet transverse dans lequel vous allez pouvoir mettre des Data Analystes qui sont beaucoup plus habitués à manipuler des business, des développeurs, et vous allez pouvoir leur faire parler un langage qu'ils parlent ensemble, qui sont les data, pour arriver à trouver des solutions. Vous allez pouvoir aussi parfois tomber sur des conclusions qui ne sont pas des conclusions avec lesquelles on est totalement tous d'accord à chaque fois. Ici, à savoir que par exemple, c'est peut-être pas le bon moment pour faire de la Web Perf. Peut-être vous allez arriver à la conclusion que en fait, ce que les data disent, c'est concrètement : "Il faut d'abord plus s'engager les utilisateurs, et quand on aura un meilleur taux de conversion, alors à ce moment-là, on fera un sujet d'optimisation de la Web Performance." Et c'est possible, et dans ce cas-là, vous allez volontairement dégrader la Web Perf en mettant des choses qui sont plus intéressantes pour les utilisateurs. Et une fois que vous aurez votre maximum d'engagement, alors là, vous allez essayer d'optimiser cette page en gardant exactement le même contenu.

Évidemment, vous pouvez pas toujours parler d'argent. Vous êtes peut-être en train de travailler dans une organisation qui n'est pas une organisation e-commerce, qui n'est pas une organisation commerciale du tout, qui fait des outils qui sont très importants, notamment pour les commons, et qui ont besoin d'être optimisés. Concrètement, moi, si je vais sur un site pour aller déclarer un incendie, je veux aussi que le site soit rapide. Et si vous êtes dans cette démarche-là, du coup, il faut arriver à approcher avec un autre type de valeur. Donc, vous, vous pouvez utiliser une valeur qui est numérique, mais non monétaire, et puis vous pouvez aussi prendre du recul et utiliser d'autres types de valeurs qui peuvent être les valeurs intrinsèques de votre entreprise ou les objectifs stratégiques. Si vous avez par exemple une entreprise qui est en train de travailler à fond ça, marque employeur, bah, la Web Perf, c'est un excellent sujet, puisque c'est un sujet dans lequel les gens peuvent s'investir, apprendre des compétences qui sont durables. Il y a aucune bonne pratique de Web Perf qu'on donne aujourd'hui qu'on n'a pas déjà donné il y a 10 ans. Concrètement, c'est vraiment un sujet d'apprentissage du web qui est très viable. Vous pouvez aussi travailler votre image de marque, puisque du coup, même si vous êtes sur un site dont vous savez qu'il est par défaut lent, et vous les voulez lent parce qu'il a une expérience utilisateur très expérience et très riche. Il empêche que si cette expérience est meilleure, concrètement, elle sera mieux perçue. Et puis, évidemment, vous pouvez aussi corréler vous donner que vous allez collecter avec des données de collecte par exemple des outils de Voice of Customer où les gens expriment leur opinion, et j'insiste sur exprimer leur opinion. Et je pense que c'est pas un Anthony qui me dira le contraire. Le NPS pour ça, c'est un peu compliqué. Le NPS, n'est pas un outil d'expression de d'opinion. C'est un outil qui peut être utilisé parce qu'il y a une valeur en effet qui est numérique, mais on pourrait discuter longtemps de la valeur de la pertinence de cette valeur et la manière dont elle est collectée. Je dirais juste un truc : les gens qui mettent un mauvais NPS, c'est des gens qui tiennent à votre marque quand même, parce que sinon, ils prendraient même pas le temps de le faire. Alors, qu'est-ce que c'est, une valeur idéale ? Une fois qu'on a dit ça, moi, j'en ai une que j'aime beaucoup et que je montre à tous mes clients. C'est un A/B test qu'on a fait pour Brico Dépôt avec la solution Fasterize. Fasterize, pour les gens qui connaîtraient pas, c'est un proxy d'optimisation de performance web à la volée. En gros, vous changez rien sur le DSI, et puis vous avez une solution intermédiaire qui capture le trafic et qui renvoie directement aux gens des pages optimisées. Si vous voulez, ça, des questions, ils ont un stand en bas. Vous pouvez leur poser des questions. Et nous, on avait ce client, Brico Dépôt, qui est venu vers nous, non pas pour nous demander si la Web Perf, c'était bien ou pas bien, mais pour nous demander si concrètement les résultats qu'on voyait avec des tests synthétiques, on pouvait les voir aussi avec du RUM et quel impact ça pouvait avoir sur les clients. Donc, du coup, on a construit un certain nombre de tableaux de bord dans lequel on a permis aux clients d'aller consulter. Là, c'est un exemple sur l'ensemble du site, mais on avait aussi des exemples un peu plus fins : le LCP, le CLS, le FID en mobile en desktop pour voir les gains. Et déjà immédiatement au moment de la livraison sur un petit test A/B avec un petit ouverture de test A/B, on a pu voir que sur le segment qui était amélioré automatiquement, on avait quand même une réduction du LCP qui était phénoménale. Il y avait des petits défauts, par exemple, CLS mobile devait être un tout petit peu dégradé, mais qui était dans des jeux de valeurs qui sont finalement très, très intéressants. En est sur une dégradation qui flirte avec la recommandation de Google. On est vraiment pas sur quelque chose d'important.

En travaillant un petit peu sur les données plutôt business, on a pu voir assez rapidement que même en ouvrant le test A/B sur une petite portion d'utilisateurs qui du coup avait des très bonnes perfs par rapport aux autres, ça suffisait à améliorer suffisamment en fait l'ensemble des indicateurs du site pour bénéficier d'un vrai surplus de trafic à SEO, trafic direct. Et on l'a vu vraiment arriver au fil des semaines avec un trafic qui progressivement était plus important, et sur ce segment, une baisse notable du taux de rebond. Les pages étant plus performantes, les gens restaient. C'était ce qu'on avait envisagé, mais c'est toujours mieux de pouvoir le collecter, de pouvoir le vérifier. Le problème, c'est que c'était pas l'ensemble de la vérité. L'ensemble de la vérité, c'est qu'en fait, en vrai, sur ce segment qui était optimisé à ce moment-là du test A/B, on avait un taux de conversion qui était très mauvais. On avait un panier moyen qui était bien plus mauvais que d'habitude, et il s'avère que nous, chez Contentsquare, on collecte aussi les erreurs JavaScript. Et on a vu en fait au bout de quelques jours que en fait, on avait un nombre d'erreurs qui étaient en augmentation. Pourquoi ? Parce que quand vous faites des optimisations Web Perf, et c'est un autre sujet que je voulais aborder, vous faites des optimisations. Donc, vous changez le site, et potentiellement, vous pouvez casser des choses que vous pouvez ne pas voir si vous n'avez pas une démarche qui est liée à ce que fait le digital et le marketing. Si vous n'avez pas regardé si les objectifs d'utilisateurs sont toujours remplis, si les parcours ont pas changé, si les gens ont pas un taux de rebond plus important, s'ils ont pas un taux de sortie plus important à l'intérieur du tunnel, concrètement, vous savez pas si en optimisant, vous avez pas trop optimisé. Et là, c'est ce qui se passait en livrant la solution. En fait, il y avait des choses que on n'avait pas vu au moment de l'implémentation à travers de l'ensemble des équipes, et notamment que il y avait certains JavaScript qui était un tout petit peu trop optimisé et qui du coup fonctionnait plus comme il devait fonctionner. Aucun problème, c'est le but d'intestiner. On détecte en amont sur une petite portion d'utilisateurs, on corrige, on relivre, et à la fin du test, et là, je parle d'un test qui a duré plusieurs mois. On parle pas quelque chose que vous faites sur une semaine. Sur un test qui a duré plusieurs mois, on avait une corrélation très, très forte entre la baisse des indicateurs de performance web à LCP qui était meilleur, un First Input Delay bien meilleur, et puis une augmentation notable des revenus, les taux de conversion et des durées de session. J'aime bien cet exemple, parce que c'était un POC vraiment pour Brico Dépôt, et ça a permis en fait à l'ensemble de la structure de prouver que concrètement, ça avait un intérêt de faire de la Web Perf. Ce qui a permis et à Fasterize de pouvoir aller défendre que c'était intéressant aussi auprès des autres marques du groupe, mais aussi en interne à une équipe web d'aller pouvoir mieux en fait son existence, mieux défendre ses budgets, mieux défendre l'ensemble des choses qui font pour que concrètement Fasterize partent de moins loin dans son optimisation. Donc, on est vraiment dans une démarche qui est vraiment vertueuse pour l'ensemble du sujet Web Perf, parce que elle a pu mettre un chiffre sur des optimisations.

Voilà, j'ai beaucoup parlé, mais si vous devez retenir trois trucs à la fin de cette intervention, c'est que concrètement, les KPI de performance qu'on regarde, les indicateurs de Web Perf, ces indicateurs qui sont extrêmement limités, on n'a pas mieux. On essaie de les faire évoluer, mais concrètement, ces indicateurs qui sont assez limités, si on veut se rendre compte vraiment de la démarche Web Perf et pouvoir pérenniser à terme des projets, à un moment donné, il va falloir qu'on mette une valeur dessus qui n'est pas nécessairement une valeur Web Perf, parce que toutes les organisations ne sont pas assez mûres pour traiter uniquement de ce sujet-là par le biais des indicateurs de perf. Donc, pour aller discuter avec des gens qui ne parlent pas indicateurs de perf et qui pensent que c'est un sujet purement IT, il faut aller parler argent. Il faut aller parler valeur, peut-être non monétaire, mais en tout cas, des valeurs qui sont partagées au sein de l'entreprise. Donc, n'hésitez pas à compléter vous chez vous avec les solutions que vous avez à votre disposition, vos Core Web Vitals avec les outils Analytics que vous avez, et vous avez forcément des Analytics quelque part dans un coin. Vous avez forcément des choses à utiliser pour regarder ce que vous faites de manière à évaluer la valeur des optimisations que vous avez faites, et faire un petit peu de blague. Il faut vous vanter. Il faut vendre le truc pour dire : "Voilà ce qu'on fait, c'est bien ou c'est pas bien, et dans ce cas-là, on passe à autre chose, parce qu'on a mieux à faire de notre temps que de faire des choses qui servent." Donc, n'hésitez pas à construire ce discours. Ça vous permettra de monter en termes de discours stratégique et d'aller parler aussi à des gens qui ont des capacités décisionnelles qui sont beaucoup plus importantes que en général c'est le développement.

Merci beaucoup.

[Applaudissements]

Appuyer sur le bouton, monsieur. C'est pas grave, je parlais plus fort. Voilà, c'est impressionnant quand je parle plus fort tout de suite.

Des questions ? Merci, merci beaucoup pour cette présentation. Je suis très attaché aux données que tu as présenté, évidemment.

Allez, on a 5 minutes de questions.

Loïc : Le hic de Google, merci, c'était hyper intéressant, la place de la valeur, la démonstration, le fait de montrer l'impact de la Web Perf. Je me dis, je sais peut-être un ajout, c'est pas une question, mais c'est : la accompagner dans le temps, c'est-à-dire que nous, on a, voilà, toutes les, on accompagne pas mal de clients sur ces sujets-là, et ce qui est difficile aussi, c'est de maintenir le sujet de la voix de performance dans le temps, parce que des fois, on va faire un A/B test, on va prouver, on va mettre en place des recommandations, on va dire que c'est fait, et puis, et ça, on le voit chez beaucoup de nos clients, et après, en fait, un an après, il y a le site vie. Donc, il évolue, et on va avoir de nouveau des régressions, et je trouve que c'est pas forcément évident de maintenir dans effectivement dans ces sujets, la gouvernance, de maintenir ça dans le temps. Donc, je vais un peu un watch out, et je sais pas s'il y a des raisons, et c'est ce que je montrais en distinguant vraiment le sujet opérationnel, sujet gouvernance, la BTS. Concrètement, le juste la BTS, c'est un sujet opérationnel. On a fait un test, c'était du run, on a prouvé un truc, ça marche. Derrière, il faut arriver à avancer sur les autres sujets. Si on s'arrête avec nos clients à la BTS, si concrètement, ils ne savent pas le reproduire, ils savent pas choisir eux-mêmes les optimisations, ils savent pas les valoriser, ils savent pas les discuter budgétairement, et ils savent pas organiser la culture autour de ces organisations-là, de ces optimisations-là, concrètement, en effet, je suis d'accord avec toi. Un an après, tu reviens chez le client, il se passe plus rien, et c'est fini. Donc, c'est aussi pour ça que moi, mon job, et c'est ce que je disais en introduction, c'est littéralement d'accompagner les clients jusqu'à ce qu'ils soient autonomes et récurrents sur ces actions-là. Je dis pas que j'y arrive, c'est un autre sujet, mais il y a quand même pas mal de mes clients dans la salle qui, a priori, s'ils sont là, sont intéressés par le sujet, et donc, ils font quand même un peu de taf de ce côté-là, mais ouais, c'est un sujet qui est très compliqué, évidemment, mais qui de toute façon, peut pas démarrer si à la base, il y a pas au moins une preuve de valeur. Donc, ça change rien sur le fait qu'il faut une preuve de valeur, mais en effet, derrière, il faut la travailler à fond, quoi, et là, typiquement, ce cas qu'on a pour Brico Dépôt, on l'a fait il y a un an, et j'en parle toujours. C'est important, un peu moins.

Est-ce qu'il y a une autre question ?

Merci beaucoup, Boris. Julien de Stratexio. J'en profite d'être juste à côté. Question plus spécifique : on va dire, est-ce qu'on sait concrètement quelles actions font qu'une page recharge ? Tu parlais tout à l'heure de fermer le PC, c'est pour ça qu'on prend les landing pages, mais du coup, est-ce qu'on sait clairement qu'est-ce qui fait qu'une page va recharger ses indicateurs ? Est-ce qu'on sait techniquement ce qui fait qu'une page recharge ?

Oui et non. On sait d'où viennent tous les rechargements, non. On fait des corrélations parfois. On en a notamment sur notre outil d'erreur analysis qui collecte les erreurs JS pour essayer de corréler en fait un certain nombre d'événements par rapport à d'autres événements, et donc on va être capable de te dire que en effet, les gens qui ont des frustrations d'interaction, par exemple, qui cliquent sur un bouton, puis ça marche pas, et ben, ils vont avoir tendance à recharger la page. Est-ce que c'est la seule raison ? Non, tu peux littéralement avoir des gens qui juste quittent leur ordinateur parce que ils vont faire autre chose. Le petit dernier est tombé de la chaise, hop, et puis le temps que tu reviennes, demi-heure plus tard, concrètement, c'est une nouvelle session. Peut-être tu as été déconnecté, donc tu vas te reconnecter, peut-être tu vas recharger ta page. Tu peux avoir fermé ton ordi et ouvert, tu peux être parti, et parce que tu utilises ton ordi professionnel et qui a une politique spécifique d'interaction, ton ordi, c'est mis en veille automatiquement, et du coup, tu reviens, ça recharge. Il y a plein de raisons qui font que la page va se recharger. Techniquement, normalement, ça devrait pas arriver tout seul. Il y a toujours en général au moins une raison. Après, de la trouver, c'est très, très compliqué, et ça biaise énormément dans le cas des notamment des Single Page Applications, parce que du coup, on se retrouve à collecter des données de Core Web Vitals dans des Single Page Applications qui devraient pas en produire, parce que quand tu es étape 4 de ton tunnel, normalement, tu devrais pas produire un LCP, parce que c'est arrivé de l'étape 3, tu vas à l'étape 5, c'est tout en JavaScript, et du coup, quand tu as des données RUM, faut vraiment penser à nettoyer en fait ces données-là de tout ce qui pourrait être du bruit et qui serait pas représentatif. Il y a pas mal de conférences sur le sujet aujourd'hui. Je vous invite à aller voir notamment la conférence sur RUM de La Redoute, son qui parle un petit peu de ce sujet-là aussi. Il me semble, et Tim aussi sur le nettoyage, Nice Conceding. Donc, je pense avec la dernière fois, même moins, on va se faire vider. Merci.

Boris : Très intéressant. Arthur de Splunk, j'ai deux questions. La première, est-ce que tu as des exemples en tête des cas sur lesquels tu as utilisé l'impact opérationnel de la Web Perf dans ton analyse de valeur, et j'ai en tête particulièrement le coût induit par les appels au call center, par exemple. Ça, c'est un premier exemple. Donc, première question, les aspects opérationnels, et la deuxième question, est-ce que tu as beaucoup, ou tu vois des demandes sur de la Web Perf, mais pas sur du retail, pas sur du B2C, plus des applications business, typiquement des applications de livraison, le camion, il peut pas partir si l'application, elle marche pas, et donc on est dans un cas intra-entreprise ou inter-entreprise ?

Alors, oui et oui, mais j'ai pas de checker quel client j'ai le droit de nommer, donc je vais pas.

[Musique]

Et la deuxième question, c'était sur le B2B. Oui, on a des clients B2B qui cherchent à faire des optimisations de Web Perf, notamment sur des tunnels extrêmement complexes. On a des clients avec des tunnels, on va vous aller tomber, mais avec des tunnels à 120 étapes, parce que bah oui, il y a des business qui sont très complexes, et en effet, optimiser ça, permet de gagner des sous avec parfois des billets très particuliers. Je pense notamment à ce client qui a des utilisateurs de business qui utilisent des tablettes en fait à l'intérieur des entreprises de leurs clients, et d'un seul coup, on a eu un jour, on a fait une livraison, et puis on a eu une optimisation incroyable, mais tellement incroyable que c'était pas possible sur notre livraison, et on a fait, c'était pas notre livraison. C'était juste leur plus gros partenaire où vous avez changé tout le parc de tablettes. Donc, en effet, ça allait plus vite.

Merci.

Je crois qu'on aura pas le temps pour une dernière question. Merci moi aussi.

{% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-0.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-1.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-2.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-3.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-4.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-5.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-6.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-7.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-8.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-9.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-10.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-11.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-12.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-13.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-14.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-15.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-16.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-17.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-18.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-19.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-20.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-21.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-22.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-23.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-24.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-25.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-26.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-27.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-28.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-29.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-30.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-31.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-32.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-33.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-34.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-35.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-36.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-37.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-38.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-39.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-40.png" alt=img_alt %} {% capture img_alt %}{% endcapture %}{% include rwd-image.html.liquid path="/assets/images/web/2025-02-01/parlons-de-valeur/slide-41.png" alt=img_alt %}


---

{% include media/youtube.html.liquid id="wh0zCqZRuSs" title="FR
Parlons de valeur — Boris Schapira — We Love Speed 2023 " %}
